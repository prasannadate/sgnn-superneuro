{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate variants of CiteSEER\n",
    "\n",
    "This file generates several different smaller versions of CiteSEER.\n",
    "The goal is to create datasets that are small enough to be run on neuromorphic hardware,\n",
    "which may impose some restrictions on dataset size i.e. number of neurons, number of synapses, etc.\n",
    "\n",
    "The following variants are generated:\n",
    "\n",
    "- **MiniSEER**:\n",
    "  \n",
    "    This dataset contains only the largest connected component of CiteSEER.\n",
    "\n",
    "    MiniSEER, or **mSEER**, is a subgraph of CiteSEER.\n",
    "\n",
    "- **MicroSEER**:\n",
    "  \n",
    "    This dataset contains only the largest connected component of CiteSEER,\n",
    "    but with a smaller number of neurons. It is generated by randomly\n",
    "    removing neurons from the largest connected component while preserving\n",
    "    connectedness of the citation network.\n",
    "\n",
    "    MicroSEER, or **uSEER**, is a subgraph of *mSEER*.\n",
    "\n",
    "    Click [here](#stats) for a table of statistics for mSEER and uSEER.\n",
    "\n",
    "- **BIteSEER**:\n",
    "\n",
    "    BIteSEER is a collection of binary classification datasets.\n",
    "    Each set is a subgraph of *mSEER*, but with only papers from\n",
    "    one of two topics. We also take the largest connected component\n",
    "    from this, so each set is a connected graph.\n",
    "\n",
    "    There are 6 topics, so there are $\\binom 6 2 = 15$ binary classification datasets.\n",
    "\n",
    "    Click [here](#biteseer-stats) for a table of statistics for each BIteSEER dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import itertools as it\n",
    "from typing import Callable\n",
    "from functools import partial\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "WRITEOUT_OKAY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split_indices(papers, test_size=0.2, rng=None) -> tuple[np.ndarray[int], np.ndarray[int]]:\n",
    "    \"\"\"\n",
    "    Splits a list of papers into train and test indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    papers : iterable of papers with ids\n",
    "        List or dict of papers to split.\n",
    "    test_size : float, optional\n",
    "        if < 1, proportion of papers to reserve for testing, by default 0.2\n",
    "        if > 1, number of papers to reserve for testing\n",
    "    rng : int, optional\n",
    "        Random state for the random number generator, default uses numpy's random\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_indices : list of int\n",
    "        List of indices for the training set.\n",
    "    test_indices : list of int\n",
    "        List of indices for the testing set.\n",
    "    \"\"\"\n",
    "    n = papers if isinstance(papers, int) else len(papers)\n",
    "    if test_size < 1:\n",
    "        test_size = int(np.floor(test_size * n))  # number of papers in test\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()  # setup rng\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "    if isinstance(papers, int):\n",
    "        indices = np.arange(n)  # generate indices\n",
    "    elif isinstance(papers, dict):\n",
    "        indices = [paper.idx for paper in papers.values()]  # grab indices from dict entries\n",
    "    else:  # assume papers is a dict or mapping of papers with a .values() method\n",
    "        indices = papers  # assume papers is a list of indices\n",
    "    indices = np.asarray(indices)\n",
    "\n",
    "    # shuffle and split\n",
    "    rng.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3312 papers and skipped 15 broken references.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AI', 'Agents', 'DB', 'HCI', 'IR', 'ML'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    idx: str  # Paper ID\n",
    "    label: str  # Paper category/topic\n",
    "    features: tuple[bool | int, ...] = ()  # binary features\n",
    "    cited_by: list[str] = field(default_factory=list)  # IDs of papers that cite this paper\n",
    "    citations: list[str] = field(default_factory=list)  # IDs of papers cited by this paper\n",
    "\n",
    "\n",
    "papers = {}\n",
    "missing = set()\n",
    "\n",
    "# Load in training data\n",
    "content = pd.read_csv(\"data/citeseer/citeseer.content\", sep=\"\\t\", header=None, dtype=object)\n",
    "citations = pd.read_csv(\"data/citeseer/citeseer.cites\", sep=\"\\t\", header=None, dtype=object)\n",
    "\n",
    "labels = set()  # set of unique labels\n",
    "\n",
    "for paper in content.itertuples(index=False):  # create papers from data\n",
    "    # if paper[-1] == 'IR':\n",
    "    #     continue\n",
    "    idx = paper[0]\n",
    "    features = tuple([int(feature) for feature in paper[1:-1]])  # parse features\n",
    "    papers[idx] = Paper(idx, paper[-1], features)  # create paper object\n",
    "    labels.add(paper[-1])  # label is the last column. add to set of labels\n",
    "\n",
    "for citation, paper_idx in citations.itertuples(index=False):\n",
    "    try:  # parse citations\n",
    "        if citation not in papers:\n",
    "            missing.add(citation)  # if citation is missing, add to missing list\n",
    "            continue\n",
    "        papers[paper_idx].citations.append(citation)\n",
    "    except KeyError:\n",
    "        missing.add(paper_idx)  # if paper is missing, add to missing list\n",
    "\n",
    "print(f\"Loaded {len(papers)} papers and skipped {len(missing)} broken references.\")\n",
    "\n",
    "labels  # show labels set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3703"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def features_array(papers):\n",
    "    return np.array([paper.features for paper in papers.values()])\n",
    "\n",
    "n_features = None\n",
    "for paper in papers.values():\n",
    "    if n_features is None:\n",
    "        n_features = len(paper.features)\n",
    "    else:\n",
    "        assert len(paper.features) == n_features\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cited_by (reverse connections) which can only be done after all papers are loaded\n",
    "for paper in papers.values():\n",
    "    paper.cited_by = [other.idx for other in papers.values() if paper.idx in other.citations]\n",
    "\n",
    "all_papers = copy.deepcopy(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_undirected(papers):\n",
    "\n",
    "    components = []\n",
    "\n",
    "    for idx, paper in papers.items():\n",
    "        new_component = {idx}  # new set\n",
    "        new_component.update(paper.citations)  # add its citations\n",
    "        matching_sets = [c for c in components if c & new_component]  # find existing components with overlap\n",
    "        new_component = new_component.union(*matching_sets)  # merge overlapping components\n",
    "        for s in matching_sets:  # remove any old overlapping components\n",
    "            components.remove(s)\n",
    "        components.append(new_component)  # add our new component\n",
    "\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2110, 18, 17, 15, 13, 12, 12, 11, 10, 10, 9, 9, 9, 9, 9, 8, 8, 8, 8, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Found 438 components, the largest has 2110 papers\n"
     ]
    }
   ],
   "source": [
    "components = sorted(components_undirected(papers), key=len, reverse=True)\n",
    "largest = components[0]\n",
    "\n",
    "print([len(c) for c in components])\n",
    "print(f\"Found {len(components)} components, the largest has {len(largest)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeout(papers, path_stem, train_idxs=None, test_idxs=None, metadata=''):\n",
    "    if not WRITEOUT_OKAY:\n",
    "        return\n",
    "    path = pl.Path(path_stem)\n",
    "    if path.is_dir():\n",
    "        msg = f\"{path} is a directory\"\n",
    "        raise ValueError(msg)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path.with_suffix('.content'), 'w') as f:\n",
    "        for paper in papers.values():\n",
    "            features = [str(int(feature)) for feature in paper.features]\n",
    "            features = '\\t'.join(features)\n",
    "            f.write(f\"{paper.idx:s}\\t{features}\\t{paper.label:s}\\n\")\n",
    "    lines = []\n",
    "    for paper in papers.values():\n",
    "        for cite in paper.citations:\n",
    "            lines.append(f\"{cite:s}\\t{paper.idx:s}\\n\")\n",
    "    lines.sort()\n",
    "    with open(path.with_suffix('.cites'), 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "    del lines\n",
    "    if train_idxs is not None and test_idxs is not None:\n",
    "        train_idxs = [str(idx) for idx in train_idxs]\n",
    "        test_idxs = [str(idx) for idx in test_idxs]\n",
    "        with open(path.with_suffix('.indices'), 'w') as f:\n",
    "            f.write(f\"training_indices {len(train_idxs)} \\\\n test_indices {len(test_idxs)}\\t{metadata}\\n\")\n",
    "            f.write(', '.join(train_idxs))\n",
    "            f.write('\\n' + ', '.join(test_idxs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_broken_citations(papers):\n",
    "    removed = set()\n",
    "    for paper in papers.values():\n",
    "        to_remove = []\n",
    "        for citation in paper.citations:\n",
    "            if citation not in papers:\n",
    "                removed.add(citation)\n",
    "                to_remove.append(citation)\n",
    "        for citation in to_remove:\n",
    "            paper.citations.remove(citation)\n",
    "        to_remove = []\n",
    "        for citer in paper.cited_by:\n",
    "            if citer not in papers:\n",
    "                removed.add(citer)\n",
    "                to_remove.append(citer)\n",
    "        for citer in to_remove:\n",
    "            paper.cited_by.remove(citer)\n",
    "    return removed\n",
    "\n",
    "\n",
    "def rebuild(papers, ids):\n",
    "    new_papers = {k: v for k, v in papers.items() if k in ids}\n",
    "    removed_citations = remove_broken_citations(new_papers)\n",
    "    return new_papers, removed_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many(papers, train_idxs=None, do_print=True):\n",
    "    def printif(*args, **kwargs):\n",
    "        if do_print:\n",
    "            print(*args, **kwargs)\n",
    "\n",
    "    n_papers = len(papers)\n",
    "    n_citations = sum([len(paper.citations) for paper in papers.values()])\n",
    "    n_topics = len(set(paper.label for paper in papers.values()))\n",
    "    n_activefeatures = np.sum(features_array(papers).sum(0) >= 1)\n",
    "    printif(f\"Papers: {n_papers}, Citations: {n_citations}, Topics: {n_topics}\")\n",
    "    printif(f\"Active features: {n_activefeatures}\")\n",
    "    neurons = n_papers + n_topics\n",
    "    if train_idxs is None:\n",
    "        printif(f\"NEST-like implementation will use {neurons} neurons\")\n",
    "        return n_papers, n_citations, n_topics, n_activefeatures, neurons, None, None, None\n",
    "    else:\n",
    "        n_train, n_test = len(train_idxs), n_papers - len(train_idxs)\n",
    "        synapses = n_citations * 2 + n_test * n_topics * 2 + n_train * 2\n",
    "        printif(f\"NEST-like implementation will use {neurons} neurons, {synapses} synapses\")\n",
    "        # printif(f\"K implementation will use {n_papers + n_topics} neurons, {n_citations + n_papers} synapses\")\n",
    "        printif(f\"Train: {n_train}, Test: {n_test} ({n_train / n_papers:.2f}:{n_test / n_papers:.2f})\")\n",
    "        return n_papers, n_citations, n_topics, n_activefeatures, neurons, synapses, n_train, n_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider_papers(\n",
    "    papers: dict,\n",
    "    max_n: int,\n",
    "    seed: int | str | Paper | None = None,\n",
    "    rng: np.random.Generator | int | None = None,\n",
    "    callback: Callable[[Paper], Paper] | None = None,\n",
    "    directed=False,\n",
    "):\n",
    "    \"\"\"Create a new connected graph of papers\"\"\"\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()  # setup rng\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "    seed: int | str | Paper = rng.choice(list(papers)) if seed is None else seed\n",
    "    new_papers: dict[str | int, Paper] = {}\n",
    "    # A paper is terminal if all its citations are in new_papers\n",
    "    terminals = set()\n",
    "\n",
    "    def is_terminal(paper):\n",
    "        if isinstance(paper, (str, int)):\n",
    "            paper: Paper = new_papers[paper]\n",
    "        if not paper.citations and not paper.cited_by:\n",
    "            return True\n",
    "        return all(c in new_papers for c in paper.citations) and all(c in new_papers for c in paper.cited_by)\n",
    "\n",
    "    def add_paper(paper: str | int | Paper):\n",
    "        if isinstance(paper, (str, int)):\n",
    "            paper: Paper = papers[paper]\n",
    "        paper = copy.deepcopy(paper)\n",
    "        new_papers[paper.idx] = callback(paper) if callback else paper\n",
    "        return new_papers[paper.idx]\n",
    "\n",
    "    paper = add_paper(seed)\n",
    "    if is_terminal(paper):\n",
    "        return new_papers\n",
    "\n",
    "    while len(new_papers) < max_n:\n",
    "        pool = set(new_papers) - terminals\n",
    "        if not pool:\n",
    "            break\n",
    "        paper = new_papers[rng.choice(list(pool))]\n",
    "        if is_terminal(paper):\n",
    "            terminals.add(paper.idx)\n",
    "            continue\n",
    "        candidates = set(paper.citations) | set(paper.cited_by) - terminals\n",
    "        assert candidates\n",
    "        new_paper = add_paper(rng.choice(list(candidates)))\n",
    "        if is_terminal(new_paper):\n",
    "            terminals.add(new_paper.idx)\n",
    "\n",
    "    return new_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_undirected_minus(papers, candidate_idx):\n",
    "\n",
    "    components = []\n",
    "\n",
    "    for idx, paper in papers.items():\n",
    "        if idx == candidate_idx:\n",
    "            continue\n",
    "        new_component = {idx}  # new set\n",
    "        # pretend candidate paper doesn't exist\n",
    "        citations = [cite for cite in paper.citations if cite != candidate_idx]\n",
    "        new_component.update(citations)  # add its citations\n",
    "        matching_sets = [c for c in components if c & new_component]  # find existing components with overlap\n",
    "        new_component = new_component.union(*matching_sets)  # merge overlapping components\n",
    "        for s in matching_sets:  # remove any old overlapping components\n",
    "            components.remove(s)\n",
    "        components.append(new_component)  # add our new component\n",
    "\n",
    "    return components\n",
    "\n",
    "\n",
    "def cull_papers(\n",
    "    papers: dict,\n",
    "    min_n: int,\n",
    "    rng: np.random.Generator | int | None = None,\n",
    "):\n",
    "    \"\"\"Create a new connected subgraph of papers\"\"\"\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()  # setup rng\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "    new_papers = copy.deepcopy(papers)\n",
    "    assert len(components_undirected(new_papers)) == 1\n",
    "\n",
    "    candidates = list(new_papers)\n",
    "    while len(new_papers) > min_n and candidates:\n",
    "        # randomly select a paper\n",
    "        paper = rng.choice(candidates)\n",
    "        candidates.remove(paper)\n",
    "\n",
    "        # check if conectivity is preserved by removing this paper\n",
    "        components = components_undirected_minus(new_papers, paper)\n",
    "        if len(components) == 1:\n",
    "            # if so, remove the paper\n",
    "            new_papers.pop(paper)\n",
    "            _removed = remove_broken_citations(new_papers)\n",
    "            candidates = list(new_papers)\n",
    "\n",
    "    return new_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "Papers: 3312, Citations: 4715, Topics: 6\n",
      "Active features: 3703\n",
      "NEST-like implementation will use 3318 neurons, 42544 synapses\n",
      "Train: 663, Test: 2649 (0.20:0.80)\n",
      "\n",
      "Miniseer:\n",
      "Papers: 2110, Citations: 3757, Topics: 6\n",
      "Active features: 3604\n",
      "NEST-like implementation will use 2116 neurons, 28614 synapses\n",
      "Train: 422, Test: 1688 (0.20:0.80)\n",
      "[2110]\n",
      "Found 1 components, the largest has 2110 papers\n",
      "\n",
      "Microseer:\n",
      "Papers: 84, Citations: 86, Topics: 6\n",
      "Active features: 1227\n",
      "NEST-like implementation will use 90 neurons, 1010 synapses\n",
      "Train: 17, Test: 67 (0.20:0.80)\n",
      "[84]\n",
      "Found 1 components, the largest has 84 papers\n",
      "Removed 0 broken citations\n"
     ]
    }
   ],
   "source": [
    "seed = 2025\n",
    "\n",
    "# === Original dataset ===\n",
    "train_idxs, test_idxs = train_test_split_indices(papers, test_size=0.8, rng=seed)\n",
    "print(\"Original dataset:\")\n",
    "how_many(papers, train_idxs)\n",
    "\n",
    "# === Miniseer ===\n",
    "miniseer, removed = rebuild(papers, largest)\n",
    "assert not removed\n",
    "m_train_idxs, m_test_idxs = train_test_split_indices(miniseer, test_size=0.8, rng=seed)\n",
    "writeout(miniseer, 'data/miniseer/miniseer', m_train_idxs, m_test_idxs, f\"seed: {seed}\")\n",
    "\n",
    "print(\"\\nMiniseer:\")\n",
    "how_many(miniseer, m_train_idxs)\n",
    "minicomponents = sorted(components_undirected(miniseer), key=len, reverse=True)\n",
    "minilargest = minicomponents[0]\n",
    "print([len(c) for c in minicomponents])\n",
    "print(f\"Found {len(minicomponents)} components, the largest has {len(minilargest)} papers\")\n",
    "\n",
    "# === Microseer ===\n",
    "# microseer = spider_papers(miniseer, 84, seed='55740', rng=seed)\n",
    "# removed = remove_broken_citations(microseer)\n",
    "microseer = cull_papers(miniseer, 84, rng=seed)\n",
    "u_train_idxs, u_test_idxs = train_test_split_indices(microseer, test_size=0.8, rng=seed)\n",
    "writeout(microseer, 'data/microseer/microseer', u_train_idxs, u_test_idxs, f\"seed: {seed}\")\n",
    "# STATS\n",
    "print(\"\\nMicroseer:\")\n",
    "how_many(microseer, u_train_idxs)\n",
    "microcomponents = sorted(components_undirected(microseer), key=len, reverse=True)\n",
    "microlargest = microcomponents[0]\n",
    "print([len(c) for c in microcomponents])\n",
    "print(f\"Found {len(microcomponents)} components, the largest has {len(microlargest)} papers\")\n",
    "print(f\"Removed {len(removed)} broken citations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['55740', 'craven98learning', 'horrocks98using']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_cited_cats = {}\n",
    "\n",
    "for paper in miniseer.values():\n",
    "    cite_categories = (set(papers[other].label for other in paper.citations) |\n",
    "                       set(papers[other].label for other in paper.cited_by))\n",
    "    by_cited_cats[paper.idx] = cite_categories\n",
    "\n",
    "list(filter(lambda k: len(by_cited_cats[k]) >= 5, by_cited_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_counts(papers, labels=(), rekey=string.ascii_uppercase):\n",
    "    if labels and isinstance(labels, str):\n",
    "        labels = labels.split(' vs. ')\n",
    "    if labels:\n",
    "        rename_to = {label: letter for label, letter in zip(labels, rekey)}\n",
    "\n",
    "    topics_count = {}\n",
    "    for paper in papers.values():\n",
    "        key = rename_to[paper.label] if labels else paper.label\n",
    "        topics_count[key] = topics_count.get(key, 0) + 1\n",
    "    return topics_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AI",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Agents",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DB",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "HCI",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "IR",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ML",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "63161fab-7f3e-4764-a80f-565a406da43f",
       "rows": [
        [
         "original",
         "249",
         "596",
         "701",
         "508",
         "668",
         "590"
        ],
        [
         "miniseer",
         "115",
         "463",
         "388",
         "304",
         "532",
         "308"
        ],
        [
         "microseer",
         "3",
         "20",
         "12",
         "26",
         "15",
         "8"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AI</th>\n",
       "      <th>Agents</th>\n",
       "      <th>DB</th>\n",
       "      <th>HCI</th>\n",
       "      <th>IR</th>\n",
       "      <th>ML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>249</td>\n",
       "      <td>596</td>\n",
       "      <td>701</td>\n",
       "      <td>508</td>\n",
       "      <td>668</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miniseer</th>\n",
       "      <td>115</td>\n",
       "      <td>463</td>\n",
       "      <td>388</td>\n",
       "      <td>304</td>\n",
       "      <td>532</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microseer</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AI  Agents   DB  HCI   IR   ML\n",
       "original   249     596  701  508  668  590\n",
       "miniseer   115     463  388  304  532  308\n",
       "microseer    3      20   12   26   15    8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "categories = pd.DataFrame({\n",
    "    'original': pd.Series(topics_counts(papers)),\n",
    "    'miniseer': pd.Series(topics_counts(miniseer)),\n",
    "    'microseer': pd.Series(topics_counts(microseer)),\n",
    "}).T\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Papers')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def degree(paper, direction=None):\n",
    "    match direction.lower() if isinstance(direction, str) else direction:\n",
    "        case 'in':\n",
    "            return len(paper.cited_by)\n",
    "        case 'out':\n",
    "            return len(paper.citations)\n",
    "        case _:\n",
    "            return len(set(paper.cited_by) | set(paper.citations))\n",
    "\n",
    "\n",
    "def pagerank(paper):\n",
    "    return sum(degree(paper) / len(paper.citations) for paper in papers.values())\n",
    "\n",
    "\n",
    "def todf(papers, name):\n",
    "    df = pd.DataFrame([{'name': paper.idx, 'topic': paper.label,\n",
    "                        'citations': degree(paper, direction='both')}\n",
    "                       for paper in papers.values()])\n",
    "    df['dataset'] = name\n",
    "    return df\n",
    "\n",
    "\n",
    "data = pd.concat([\n",
    "    todf(papers, 'citeseer'),\n",
    "    todf(miniseer, 'miniseer'),\n",
    "    todf(microseer, 'microseer')])\n",
    "sns.set_theme(style=\"darkgrid\", palette=sns.color_palette(['#b3cde3', '#8c96c6', '#88419d']))\n",
    "# %matplotlib qt\n",
    "%matplotlib widget\n",
    "fig, ax = plt.subplots()\n",
    "index = data['topic'].value_counts().index\n",
    "sns.countplot(data, x='topic', hue='dataset', order=index)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Papers')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = todf(papers, 'citeseer')\n",
    "# plot_df = todf(miniseer, 'miniseer')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "# fig, ax1 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "sns.set_theme(style=\"darkgrid\", palette=sns.color_palette(['#8dd3c7', '#bebada', '#fb8072', '#80b1d3', '#fdb462', '#b3de69']))\n",
    "sns.histplot(\n",
    "    plot_df, x='citations', hue='topic', ax=ax1,\n",
    "    discrete=True,\n",
    "    multiple='stack',\n",
    "    # multiple='dodge',\n",
    "    # log_scale=True,\n",
    ")\n",
    "# ax.set_xscale('log')\n",
    "ax1.set_xlim(0, 6)\n",
    "sns.histplot(\n",
    "    plot_df, x='citations', hue='topic', ax=ax2,\n",
    "    discrete=True,\n",
    "    multiple='stack',\n",
    "    # multiple='dodge',\n",
    "    # log_scale=True,\n",
    ")\n",
    "ax1.set_ylim(0, 1400)\n",
    "ax2.set_xlim(4, 100)\n",
    "ax2.set_ylim(0, 30)\n",
    "ax1.set_ylabel('Number of Papers')\n",
    "ax2.set_ylabel('Number of Papers')\n",
    "ax1.set_xlabel('Degree')\n",
    "ax2.set_xlabel('Degree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIteSEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_papers_by_labels(papers: dict, splits=None):\n",
    "#     \"\"\"Ensure a certain number of papers per label.\"\"\"\n",
    "#     # TODO: WIP\n",
    "#     labels = set(paper.label for paper in papers.values())\n",
    "#     by_label = {  # split dataset into groups of papers with the same label\n",
    "#         label: [paper.idx for paper in papers.values() if paper.label == label]\n",
    "#         for label in labels\n",
    "#     }\n",
    "#     if splits is None and labels:\n",
    "#         splits = [1 / len(labels)] * len(labels)\n",
    "#     if not isinstance(splits, dict):\n",
    "#         splits = dict(zip(labels, splits))\n",
    "#     if sum(splits.values()) <= 1:\n",
    "#         splits = {k: int(v * )}\n",
    "\n",
    "#     new = {}\n",
    "#     for label, idxs in by_label.items():\n",
    "#         new[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI vs. Agents',\n",
       " 'AI vs. DB',\n",
       " 'AI vs. HCI',\n",
       " 'AI vs. IR',\n",
       " 'AI vs. ML',\n",
       " 'Agents vs. DB',\n",
       " 'Agents vs. HCI',\n",
       " 'Agents vs. IR',\n",
       " 'Agents vs. ML',\n",
       " 'DB vs. HCI',\n",
       " 'DB vs. IR',\n",
       " 'DB vs. ML',\n",
       " 'HCI vs. IR',\n",
       " 'HCI vs. ML',\n",
       " 'IR vs. ML']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alabels = sorted(labels)\n",
    "combos = sorted(list(it.combinations(alabels, 2)))\n",
    "[' vs. '.join(pair) for pair in combos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "A",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "B",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Papers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Citations",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topics",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Features",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Neurons",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Synapses",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Train",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Test",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9aa66e55-75d5-44bf-bed4-5adcb1e2f2e1",
       "rows": [
        [
         "AI vs. Agents",
         "6.0",
         "82.0",
         "88",
         "149",
         "2",
         "989",
         "90",
         "614",
         "18",
         "70"
        ],
        [
         "AI vs. DB",
         "1.0",
         "3.0",
         "4",
         "3",
         "2",
         "107",
         "6",
         "20",
         "1",
         "3"
        ],
        [
         "AI vs. HCI",
         "1.0",
         "87.0",
         "88",
         "122",
         "2",
         "1040",
         "90",
         "560",
         "18",
         "70"
        ],
        [
         "AI vs. IR",
         "5.0",
         "83.0",
         "88",
         "176",
         "2",
         "1066",
         "90",
         "668",
         "18",
         "70"
        ],
        [
         "AI vs. ML",
         "4.0",
         "84.0",
         "88",
         "136",
         "2",
         "1006",
         "90",
         "588",
         "18",
         "70"
        ],
        [
         "Agents vs. DB",
         "86.0",
         "2.0",
         "88",
         "144",
         "2",
         "982",
         "90",
         "604",
         "18",
         "70"
        ],
        [
         "Agents vs. HCI",
         "84.0",
         "4.0",
         "88",
         "134",
         "2",
         "1023",
         "90",
         "584",
         "18",
         "70"
        ],
        [
         "Agents vs. IR",
         "1.0",
         "87.0",
         "88",
         "275",
         "2",
         "979",
         "90",
         "866",
         "18",
         "70"
        ],
        [
         "Agents vs. ML",
         "87.0",
         "1.0",
         "88",
         "133",
         "2",
         "1013",
         "90",
         "582",
         "18",
         "70"
        ],
        [
         "DB vs. HCI",
         "88.0",
         null,
         "88",
         "119",
         "1",
         "1001",
         "89",
         "414",
         "18",
         "70"
        ],
        [
         "DB vs. IR",
         "70.0",
         "18.0",
         "88",
         "132",
         "2",
         "994",
         "90",
         "580",
         "18",
         "70"
        ],
        [
         "DB vs. ML",
         "79.0",
         "9.0",
         "88",
         "135",
         "2",
         "1068",
         "90",
         "586",
         "18",
         "70"
        ],
        [
         "HCI vs. IR",
         "2.0",
         "86.0",
         "88",
         "269",
         "2",
         "970",
         "90",
         "854",
         "18",
         "70"
        ],
        [
         "HCI vs. ML",
         "16.0",
         "72.0",
         "88",
         "115",
         "2",
         "1090",
         "90",
         "546",
         "18",
         "70"
        ],
        [
         "IR vs. ML",
         "79.0",
         "9.0",
         "88",
         "179",
         "2",
         "1028",
         "90",
         "674",
         "18",
         "70"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Papers</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Features</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Synapses</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI vs. Agents</th>\n",
       "      <td>6.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>88</td>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "      <td>989</td>\n",
       "      <td>90</td>\n",
       "      <td>614</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI vs. DB</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI vs. HCI</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88</td>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>1040</td>\n",
       "      <td>90</td>\n",
       "      <td>560</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI vs. IR</th>\n",
       "      <td>5.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>88</td>\n",
       "      <td>176</td>\n",
       "      <td>2</td>\n",
       "      <td>1066</td>\n",
       "      <td>90</td>\n",
       "      <td>668</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI vs. ML</th>\n",
       "      <td>4.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>88</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>1006</td>\n",
       "      <td>90</td>\n",
       "      <td>588</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agents vs. DB</th>\n",
       "      <td>86.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88</td>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>982</td>\n",
       "      <td>90</td>\n",
       "      <td>604</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agents vs. HCI</th>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>88</td>\n",
       "      <td>134</td>\n",
       "      <td>2</td>\n",
       "      <td>1023</td>\n",
       "      <td>90</td>\n",
       "      <td>584</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agents vs. IR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88</td>\n",
       "      <td>275</td>\n",
       "      <td>2</td>\n",
       "      <td>979</td>\n",
       "      <td>90</td>\n",
       "      <td>866</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agents vs. ML</th>\n",
       "      <td>87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>1013</td>\n",
       "      <td>90</td>\n",
       "      <td>582</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DB vs. HCI</th>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>89</td>\n",
       "      <td>414</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DB vs. IR</th>\n",
       "      <td>70.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>88</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>994</td>\n",
       "      <td>90</td>\n",
       "      <td>580</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DB vs. ML</th>\n",
       "      <td>79.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>88</td>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>1068</td>\n",
       "      <td>90</td>\n",
       "      <td>586</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCI vs. IR</th>\n",
       "      <td>2.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>88</td>\n",
       "      <td>269</td>\n",
       "      <td>2</td>\n",
       "      <td>970</td>\n",
       "      <td>90</td>\n",
       "      <td>854</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCI vs. ML</th>\n",
       "      <td>16.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>88</td>\n",
       "      <td>115</td>\n",
       "      <td>2</td>\n",
       "      <td>1090</td>\n",
       "      <td>90</td>\n",
       "      <td>546</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IR vs. ML</th>\n",
       "      <td>79.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>88</td>\n",
       "      <td>179</td>\n",
       "      <td>2</td>\n",
       "      <td>1028</td>\n",
       "      <td>90</td>\n",
       "      <td>674</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A     B  Papers  Citations  Topics  Features  Neurons  \\\n",
       "AI vs. Agents    6.0  82.0      88        149       2       989       90   \n",
       "AI vs. DB        1.0   3.0       4          3       2       107        6   \n",
       "AI vs. HCI       1.0  87.0      88        122       2      1040       90   \n",
       "AI vs. IR        5.0  83.0      88        176       2      1066       90   \n",
       "AI vs. ML        4.0  84.0      88        136       2      1006       90   \n",
       "Agents vs. DB   86.0   2.0      88        144       2       982       90   \n",
       "Agents vs. HCI  84.0   4.0      88        134       2      1023       90   \n",
       "Agents vs. IR    1.0  87.0      88        275       2       979       90   \n",
       "Agents vs. ML   87.0   1.0      88        133       2      1013       90   \n",
       "DB vs. HCI      88.0   NaN      88        119       1      1001       89   \n",
       "DB vs. IR       70.0  18.0      88        132       2       994       90   \n",
       "DB vs. ML       79.0   9.0      88        135       2      1068       90   \n",
       "HCI vs. IR       2.0  86.0      88        269       2       970       90   \n",
       "HCI vs. ML      16.0  72.0      88        115       2      1090       90   \n",
       "IR vs. ML       79.0   9.0      88        179       2      1028       90   \n",
       "\n",
       "                Synapses  Train  Test  \n",
       "AI vs. Agents        614     18    70  \n",
       "AI vs. DB             20      1     3  \n",
       "AI vs. HCI           560     18    70  \n",
       "AI vs. IR            668     18    70  \n",
       "AI vs. ML            588     18    70  \n",
       "Agents vs. DB        604     18    70  \n",
       "Agents vs. HCI       584     18    70  \n",
       "Agents vs. IR        866     18    70  \n",
       "Agents vs. ML        582     18    70  \n",
       "DB vs. HCI           414     18    70  \n",
       "DB vs. IR            580     18    70  \n",
       "DB vs. ML            586     18    70  \n",
       "HCI vs. IR           854     18    70  \n",
       "HCI vs. ML           546     18    70  \n",
       "IR vs. ML            674     18    70  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bites using spidering method:\n",
    "def filter_citations(paper: Paper, labels):\n",
    "    paper.citations = [other for other in paper.citations if papers[other].label in labels]\n",
    "    paper.cited_by = [other for other in paper.cited_by if papers[other].label in labels]\n",
    "    return paper\n",
    "\n",
    "\n",
    "bites = {}\n",
    "\n",
    "for combo in combos:\n",
    "    new_idxs = [paper.idx for paper in miniseer.values() if paper.label in combo]\n",
    "    cited_cats = {paper_idx: set(papers[other].label for other in papers[paper_idx].citations) |\n",
    "                             set(papers[other].label for other in papers[paper_idx].cited_by)\n",
    "                             for paper_idx in new_idxs}\n",
    "    candidate_seeds = list(filter(lambda k: set(combo) >= cited_cats[k], cited_cats))\n",
    "    seed_paper = np.random.default_rng(seed).choice(candidate_seeds)\n",
    "    bite = spider_papers(miniseer, 88, seed=seed_paper, rng=seed, callback=partial(filter_citations, labels=combo))\n",
    "    _removed = remove_broken_citations(bite)\n",
    "    new_train_idxs, new_test_idxs = train_test_split_indices(bite, test_size=0.8, rng=seed)\n",
    "\n",
    "    bites[' vs. '.join(combo)] = (bite, new_train_idxs, new_test_idxs)\n",
    "\n",
    "\n",
    "categories = pd.DataFrame({k: pd.Series(topics_counts(v[0], k)) for k, v in bites.items()}).T\n",
    "cols = ['Papers', 'Citations', 'Topics', 'Features', 'Neurons', 'Synapses', 'Train', 'Test']\n",
    "stats = pd.DataFrame(columns=cols)\n",
    "for key, (dataset, train_idxs, _test_idxs) in bites.items():\n",
    "    n_papers, n_citations, n_topics, ftrs, neurons, synapses, n_train, n_test = how_many(dataset, train_idxs, do_print=False)\n",
    "    stats.loc[key] = pd.Series([n_papers, n_citations, n_topics, ftrs, neurons, synapses, n_train, n_test], index=cols)\n",
    "\n",
    "pd.concat([categories, stats], axis=1)\n",
    "# pd.concat([categories, pd.Series({'C': 999, 'D': 0}, name='ML vs. DB')], axis=1)\n",
    "# categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIteSEER Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "A",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "B",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Papers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Citations",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topics",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Features",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Neurons",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Synapses",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Train",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Test",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9ce9e4d6-b96c-4b11-903f-c3750439f891",
       "rows": [
        [
         "AI vs. Agents",
         "52",
         "406",
         "458",
         "708",
         "2",
         "2233",
         "460",
         "3064",
         "92",
         "366"
        ],
        [
         "AI vs. DB",
         "14",
         "257",
         "271",
         "433",
         "2",
         "1724",
         "273",
         "1840",
         "55",
         "216"
        ],
        [
         "AI vs. HCI",
         "7",
         "207",
         "214",
         "283",
         "2",
         "1581",
         "216",
         "1336",
         "43",
         "171"
        ],
        [
         "AI vs. IR",
         "20",
         "447",
         "467",
         "1019",
         "2",
         "2299",
         "469",
         "3718",
         "94",
         "373"
        ],
        [
         "AI vs. ML",
         "13",
         "150",
         "163",
         "259",
         "2",
         "1372",
         "165",
         "1104",
         "33",
         "130"
        ],
        [
         "Agents vs. DB",
         "388",
         "294",
         "682",
         "1073",
         "2",
         "2659",
         "684",
         "4600",
         "137",
         "545"
        ],
        [
         "Agents vs. HCI",
         "405",
         "262",
         "667",
         "980",
         "2",
         "2609",
         "669",
         "4360",
         "134",
         "533"
        ],
        [
         "Agents vs. IR",
         "396",
         "456",
         "852",
         "1613",
         "2",
         "2924",
         "854",
         "6292",
         "171",
         "681"
        ],
        [
         "Agents vs. ML",
         "393",
         "172",
         "565",
         "883",
         "2",
         "2513",
         "567",
         "3800",
         "113",
         "452"
        ],
        [
         "DB vs. HCI",
         "259",
         "213",
         "472",
         "694",
         "2",
         "2299",
         "474",
         "3086",
         "95",
         "377"
        ],
        [
         "DB vs. IR",
         "306",
         "485",
         "791",
         "1553",
         "2",
         "2783",
         "793",
         "5952",
         "159",
         "632"
        ],
        [
         "DB vs. ML",
         "261",
         "154",
         "415",
         "662",
         "2",
         "2209",
         "417",
         "2818",
         "83",
         "332"
        ],
        [
         "HCI vs. IR",
         "235",
         "452",
         "687",
         "1317",
         "2",
         "2722",
         "689",
         "5106",
         "138",
         "549"
        ],
        [
         "HCI vs. ML",
         "209",
         "153",
         "362",
         "527",
         "2",
         "2098",
         "364",
         "2356",
         "73",
         "289"
        ],
        [
         "IR vs. ML",
         "465",
         "214",
         "679",
         "1474",
         "2",
         "2641",
         "681",
         "5392",
         "136",
         "543"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Papers</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Features</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Synapses</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI vs. Agents</th>\n",
       "      <td>52</td>\n",
       "      <td>406</td>\n",
       "      <td>458</td>\n",
       "      <td>708</td>\n",
       "      <td>2</td>\n",
       "      <td>2233</td>\n",
       "      <td>460</td>\n",
       "      <td>3064</td>\n",
       "      <td>92</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI vs. DB</th>\n",
       "      <td>14</td>\n",
       "      <td>257</td>\n",
       "      <td>271</td>\n",
       "      <td>433</td>\n",
       "      <td>2</td>\n",
       "      <td>1724</td>\n",
       "      <td>273</td>\n",
       "      <td>1840</td>\n",
       "      <td>55</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI vs. HCI</th>\n",
       "      <td>7</td>\n",
       "      <td>207</td>\n",
       "      <td>214</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>1581</td>\n",
       "      <td>216</td>\n",
       "      <td>1336</td>\n",
       "      <td>43</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI vs. IR</th>\n",
       "      <td>20</td>\n",
       "      <td>447</td>\n",
       "      <td>467</td>\n",
       "      <td>1019</td>\n",
       "      <td>2</td>\n",
       "      <td>2299</td>\n",
       "      <td>469</td>\n",
       "      <td>3718</td>\n",
       "      <td>94</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI vs. ML</th>\n",
       "      <td>13</td>\n",
       "      <td>150</td>\n",
       "      <td>163</td>\n",
       "      <td>259</td>\n",
       "      <td>2</td>\n",
       "      <td>1372</td>\n",
       "      <td>165</td>\n",
       "      <td>1104</td>\n",
       "      <td>33</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agents vs. DB</th>\n",
       "      <td>388</td>\n",
       "      <td>294</td>\n",
       "      <td>682</td>\n",
       "      <td>1073</td>\n",
       "      <td>2</td>\n",
       "      <td>2659</td>\n",
       "      <td>684</td>\n",
       "      <td>4600</td>\n",
       "      <td>137</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agents vs. HCI</th>\n",
       "      <td>405</td>\n",
       "      <td>262</td>\n",
       "      <td>667</td>\n",
       "      <td>980</td>\n",
       "      <td>2</td>\n",
       "      <td>2609</td>\n",
       "      <td>669</td>\n",
       "      <td>4360</td>\n",
       "      <td>134</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agents vs. IR</th>\n",
       "      <td>396</td>\n",
       "      <td>456</td>\n",
       "      <td>852</td>\n",
       "      <td>1613</td>\n",
       "      <td>2</td>\n",
       "      <td>2924</td>\n",
       "      <td>854</td>\n",
       "      <td>6292</td>\n",
       "      <td>171</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agents vs. ML</th>\n",
       "      <td>393</td>\n",
       "      <td>172</td>\n",
       "      <td>565</td>\n",
       "      <td>883</td>\n",
       "      <td>2</td>\n",
       "      <td>2513</td>\n",
       "      <td>567</td>\n",
       "      <td>3800</td>\n",
       "      <td>113</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DB vs. HCI</th>\n",
       "      <td>259</td>\n",
       "      <td>213</td>\n",
       "      <td>472</td>\n",
       "      <td>694</td>\n",
       "      <td>2</td>\n",
       "      <td>2299</td>\n",
       "      <td>474</td>\n",
       "      <td>3086</td>\n",
       "      <td>95</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DB vs. IR</th>\n",
       "      <td>306</td>\n",
       "      <td>485</td>\n",
       "      <td>791</td>\n",
       "      <td>1553</td>\n",
       "      <td>2</td>\n",
       "      <td>2783</td>\n",
       "      <td>793</td>\n",
       "      <td>5952</td>\n",
       "      <td>159</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DB vs. ML</th>\n",
       "      <td>261</td>\n",
       "      <td>154</td>\n",
       "      <td>415</td>\n",
       "      <td>662</td>\n",
       "      <td>2</td>\n",
       "      <td>2209</td>\n",
       "      <td>417</td>\n",
       "      <td>2818</td>\n",
       "      <td>83</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCI vs. IR</th>\n",
       "      <td>235</td>\n",
       "      <td>452</td>\n",
       "      <td>687</td>\n",
       "      <td>1317</td>\n",
       "      <td>2</td>\n",
       "      <td>2722</td>\n",
       "      <td>689</td>\n",
       "      <td>5106</td>\n",
       "      <td>138</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCI vs. ML</th>\n",
       "      <td>209</td>\n",
       "      <td>153</td>\n",
       "      <td>362</td>\n",
       "      <td>527</td>\n",
       "      <td>2</td>\n",
       "      <td>2098</td>\n",
       "      <td>364</td>\n",
       "      <td>2356</td>\n",
       "      <td>73</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IR vs. ML</th>\n",
       "      <td>465</td>\n",
       "      <td>214</td>\n",
       "      <td>679</td>\n",
       "      <td>1474</td>\n",
       "      <td>2</td>\n",
       "      <td>2641</td>\n",
       "      <td>681</td>\n",
       "      <td>5392</td>\n",
       "      <td>136</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A    B  Papers  Citations  Topics  Features  Neurons  \\\n",
       "AI vs. Agents    52  406     458        708       2      2233      460   \n",
       "AI vs. DB        14  257     271        433       2      1724      273   \n",
       "AI vs. HCI        7  207     214        283       2      1581      216   \n",
       "AI vs. IR        20  447     467       1019       2      2299      469   \n",
       "AI vs. ML        13  150     163        259       2      1372      165   \n",
       "Agents vs. DB   388  294     682       1073       2      2659      684   \n",
       "Agents vs. HCI  405  262     667        980       2      2609      669   \n",
       "Agents vs. IR   396  456     852       1613       2      2924      854   \n",
       "Agents vs. ML   393  172     565        883       2      2513      567   \n",
       "DB vs. HCI      259  213     472        694       2      2299      474   \n",
       "DB vs. IR       306  485     791       1553       2      2783      793   \n",
       "DB vs. ML       261  154     415        662       2      2209      417   \n",
       "HCI vs. IR      235  452     687       1317       2      2722      689   \n",
       "HCI vs. ML      209  153     362        527       2      2098      364   \n",
       "IR vs. ML       465  214     679       1474       2      2641      681   \n",
       "\n",
       "                Synapses  Train  Test  \n",
       "AI vs. Agents       3064     92   366  \n",
       "AI vs. DB           1840     55   216  \n",
       "AI vs. HCI          1336     43   171  \n",
       "AI vs. IR           3718     94   373  \n",
       "AI vs. ML           1104     33   130  \n",
       "Agents vs. DB       4600    137   545  \n",
       "Agents vs. HCI      4360    134   533  \n",
       "Agents vs. IR       6292    171   681  \n",
       "Agents vs. ML       3800    113   452  \n",
       "DB vs. HCI          3086     95   377  \n",
       "DB vs. IR           5952    159   632  \n",
       "DB vs. ML           2818     83   332  \n",
       "HCI vs. IR          5106    138   549  \n",
       "HCI vs. ML          2356     73   289  \n",
       "IR vs. ML           5392    136   543  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bites using culling method:\n",
    "\n",
    "bites = {}\n",
    "\n",
    "for combo in combos:\n",
    "    new_idxs = [paper.idx for paper in miniseer.values() if paper.label in combo]\n",
    "    bite, _removed = rebuild(copy.deepcopy(miniseer), new_idxs)  # rebuild after removing papers not in combo\n",
    "    largest_component = sorted(components_undirected(bite), key=len, reverse=True)[0]\n",
    "    bite, _removed = rebuild(bite, largest_component)  # rebuild after removing all but largest component\n",
    "    # bite = cull_papers(bite, 88, rng=seed)\n",
    "    new_train_idxs, new_test_idxs = train_test_split_indices(bite, test_size=0.8, rng=seed)\n",
    "\n",
    "    bites[' vs. '.join(combo)] = (bite, new_train_idxs, new_test_idxs)\n",
    "\n",
    "\n",
    "for key, (dataset, train_idxs, test_idxs) in bites.items():\n",
    "    name = key.replace(' vs. ', '_')\n",
    "    writeout(dataset, f'data/biteseer/{name}', train_idxs, test_idxs, f\"seed: {seed}\")\n",
    "\n",
    "\n",
    "categories = pd.DataFrame({k: pd.Series(topics_counts(v[0], k)) for k, v in bites.items()}).T\n",
    "cols = ['Papers', 'Citations', 'Topics', 'Features', 'Neurons', 'Synapses', 'Train', 'Test']\n",
    "stats = pd.DataFrame(columns=cols)\n",
    "for key, (dataset, train_idxs, _test_idxs) in bites.items():\n",
    "    n_papers, n_citations, n_topics, ftrs, neurons, synapses, n_train, n_test = how_many(dataset, train_idxs, do_print=False)\n",
    "    stats.loc[key] = pd.Series([n_papers, n_citations, n_topics, ftrs, neurons, synapses, n_train, n_test], index=cols)\n",
    "\n",
    "pd.concat([categories, stats], axis=1)\n",
    "# pd.concat([categories, pd.Series({'C': 999, 'D': 0}, name='ML vs. DB')], axis=1)\n",
    "# categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Agents': '#8dd3c7',\n",
       " 'IR': '#bebada',\n",
       " 'DB': '#fb8072',\n",
       " 'AI': '#80b1d3',\n",
       " 'HCI': '#fdb462',\n",
       " 'ML': '#b3de69'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# palette = ['#8dd3c7', '#bebada', '#fb8072', '#80b1d3', '#fdb462', '#b3de69']\n",
    "# colormap = dict(zip(alabels, palette))\n",
    "colormap = {\n",
    "    'Agents': '#8dd3c7',  # aqua\n",
    "    'IR': '#bebada',  # lavender\n",
    "    'DB': '#fb8072',  # red\n",
    "    'AI': '#80b1d3',  # blue\n",
    "    'HCI': '#fdb462',  # orange\n",
    "    'ML': '#b3de69',  # green\n",
    "}\n",
    "colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_graphml(papers, name):\n",
    "    import networkx as nx\n",
    "    citenx = nx.from_edgelist((paper.idx, cite) for paper in papers.values() for cite in paper.citations)\n",
    "    for paper in papers.values():\n",
    "        citenx.nodes[paper.idx]['label'] = paper.label\n",
    "        citenx.nodes[paper.idx]['color'] = colormap[paper.label]\n",
    "    if WRITEOUT_OKAY:\n",
    "        nx.write_graphml(citenx, f'data/{name}.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphml(papers, name='citeseer/citeseer_viz')\n",
    "export_graphml(microseer, name='microseer/microseer_viz')\n",
    "export_graphml(bites['DB vs. HCI'][0], name='biteseer/DB_HCI')\n",
    "export_graphml(bites['HCI vs. ML'][0], name='biteseer/HCI_ML')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superneuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
