{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import superneuromat as snm\n",
    "from superneuromat import SNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import typing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "# from: https://stackoverflow.com/a/21894086/2712730\n",
    "class bidict(dict):\n",
    "    \"\"\"Creates a dictionary that supports reverse lookups via the .inverse attribute.\n",
    "\n",
    "    Args:\n",
    "        dict (dict): The original dictionary.\n",
    "\n",
    "    Properties:\n",
    "        inverse (dict): A dictionary that maps values to keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(bidict, self).__init__(*args, **kwargs)\n",
    "        self.inverse = {}\n",
    "        for key, value in self.items():\n",
    "            self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self:\n",
    "            self.inverse[self[key]].remove(key)\n",
    "        super(bidict, self).__setitem__(key, value)\n",
    "        self.inverse.setdefault(value, []).append(key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        self.inverse.setdefault(self[key], []).remove(key)\n",
    "        if self[key] in self.inverse and not self.inverse[self[key]]:\n",
    "            del self.inverse[self[key]]\n",
    "        super(bidict, self).__delitem__(key)\n",
    "\n",
    "\n",
    "def train_test_split_indices(papers, test_size=0.2, rng=None) -> tuple[npt.NDArray[np.generic], npt.NDArray[np.generic]]:\n",
    "    \"\"\"\n",
    "    Splits a list of papers into train and test indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    papers : iterable of papers with ids\n",
    "        List or dict of papers to split.\n",
    "    test_size : float, optional\n",
    "        if < 1, proportion of papers to reserve for testing, by default 0.2\n",
    "        if > 1, number of papers to reserve for testing\n",
    "    rng : int, optional\n",
    "        Random state for the random number generator, default uses numpy's random\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_indices : list of int\n",
    "        List of indices for the training set.\n",
    "    test_indices : list of int\n",
    "        List of indices for the testing set.\n",
    "    \"\"\"\n",
    "    n = papers if isinstance(papers, int) else len(papers)\n",
    "    if test_size < 1:\n",
    "        test_size = int(np.floor(test_size * n))  # number of papers in test\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()  # setup rng\n",
    "    elif isinstance(rng, int):\n",
    "        rng = np.random.default_rng(rng)\n",
    "\n",
    "    if isinstance(papers, int):\n",
    "        indices = np.arange(n)  # generate indices\n",
    "    elif isinstance(papers, (list, tuple)):\n",
    "        indices = papers  # assume papers is a list of indices\n",
    "    else:  # assume papers is a dict or mapping of papers with a .values() method\n",
    "        indices = [paper.idx for paper in papers.values()]  # grab indices from dict entries\n",
    "    indices = np.asarray(indices)\n",
    "\n",
    "    # shuffle and split\n",
    "    rng.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    return train_indices, test_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CiteSeer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2708 papers and skipped 0 broken references.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Case_Based',\n",
       " 'Genetic_Algorithms',\n",
       " 'Neural_Networks',\n",
       " 'Probabilistic_Methods',\n",
       " 'Reinforcement_Learning',\n",
       " 'Rule_Learning',\n",
       " 'Theory']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    idx: str  # Paper ID\n",
    "    label: str  # Paper category/topic\n",
    "    features: tuple[bool | int | float, ...] = ()  # binary features\n",
    "    citations: list[str] = field(default_factory=list)  # IDs of papers cited by this paper\n",
    "\n",
    "\n",
    "papers = {}\n",
    "missing = set()\n",
    "\n",
    "# Load in training data\n",
    "content = pd.read_csv(\"data/Cora/cora/cora.content\", sep=\"\\t\", header=None, dtype=object)\n",
    "citations = pd.read_csv(\"data/Cora/cora/cora.cites\", sep=\"\\t\", header=None, dtype=object)\n",
    "\n",
    "labels = set()  # set of unique labels\n",
    "\n",
    "for paper in content.itertuples(index=False):  # create papers from data\n",
    "    idx = paper[0]\n",
    "    features = tuple([int(feature) for feature in paper[1:-1]])  # parse features\n",
    "    papers[idx] = Paper(idx, paper[-1], features)  # create paper object\n",
    "    labels.add(paper[-1])  # label is the last column. add to set of labels\n",
    "\n",
    "for paper_idx, citation in citations.itertuples(index=False):\n",
    "    try:  # parse citations\n",
    "        if citation not in papers:\n",
    "            missing.add(citation)  # if citation is missing, add to missing list\n",
    "            continue\n",
    "        if citation in papers[paper_idx].citations:\n",
    "            continue  # skip if citation is already in citations\n",
    "        papers[paper_idx].citations.append(citation)\n",
    "    except KeyError:\n",
    "        missing.add(paper_idx)  # if paper is missing, add to missing list\n",
    "\n",
    "print(f\"Loaded {len(papers)} papers and skipped {len(missing)} broken references.\")\n",
    "labels = list(sorted(labels))  # sort labels\n",
    "# TODO: WHY DOES THIS MATTER? Set order is random, depending on interpreter hash seed.\n",
    "# but why does this give me different results if I have randommized weights...\n",
    "\n",
    "labels  # show labels set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train / test split  \n",
    "Create two lists of IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = train_test_split_indices(papers, test_size=0.2, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our model\n",
    "model = SNN()\n",
    "\n",
    "# Create our output neurons, set threshold very high so that we control when they spike during training.\n",
    "# dict mapping {category: neuron_id}\n",
    "lbl_threshold = 1.05\n",
    "strong_connection = 5.0\n",
    "weak_connection = 1.0\n",
    "unknown_connection = 0.00001\n",
    "# unknown_connection = 0.0\n",
    "\n",
    "lbl_neurons = bidict({label: model.create_neuron(threshold=lbl_threshold, leak=0).idx for label in labels})\n",
    "\n",
    "# Create our input neurons, one for each pixel of the image resolution.\n",
    "paper_neurons = bidict()  # dict mapping {paper_id: neuron_id}\n",
    "\n",
    "# make a neuron for each paper\n",
    "for paper in papers.values():\n",
    "    paper_neurons[paper.idx] = neuron_id = model.create_neuron(1, 0.).idx\n",
    "\n",
    "    if paper.idx in train_idxs:\n",
    "        # Make an explicitly STRONG synapse connecting the input to the output\n",
    "        output_id = lbl_neurons[paper.label]  # (training paper to topic)\n",
    "        model.create_synapse(neuron_id, output_id, weight=strong_connection, stdp_enabled=False, delay=1)\n",
    "        model.create_synapse(output_id, neuron_id, weight=strong_connection, stdp_enabled=False, delay=1)\n",
    "    else:  # test set, connect this input neuron to all output neurons\n",
    "        # Connect our input neuron to output neurons (test paper to topic)\n",
    "        for output_id in lbl_neurons.values():\n",
    "            # Randomize initial weight\n",
    "            weight1 = (rng.choice([-1, 1]) * unknown_connection) + 1\n",
    "            weight2 = (rng.choice([-1, 1]) * unknown_connection) + 1\n",
    "            # Make a synapse connecting the input to the output\n",
    "            model.create_synapse(neuron_id, output_id, weight=weight1, stdp_enabled=True, delay=1)\n",
    "            model.create_synapse(output_id, neuron_id, weight=weight2, stdp_enabled=True, delay=1)\n",
    "\n",
    "# connect papers by their citations (paper to paper connections)\n",
    "for paper in papers.values():\n",
    "    for citation in paper.citations:\n",
    "        if paper.idx == citation:\n",
    "            continue  # don't cite yourself\n",
    "        try:\n",
    "            model.create_synapse(paper_neurons[paper.idx], paper_neurons[citation], weight=weak_connection, stdp_enabled=True, delay=1)  # noqa\n",
    "            model.create_synapse(paper_neurons[citation], paper_neurons[paper.idx], weight=weak_connection, stdp_enabled=True, delay=1)  # noqa\n",
    "        except RuntimeError:\n",
    "            continue  # skip if citation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loop through our dataset and add spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 0\n",
    "train_idxs_augmented = np.concat((train_idxs, ) * 3)\n",
    "for idx in train_idxs_augmented:\n",
    "    paper = papers[idx]  # get paper by id\n",
    "    # Add a spike that will exceed the threshold for the respective label neuron\n",
    "    model.add_spike(timestep + 1, lbl_neurons[paper.label], strong_connection + 1)\n",
    "    # Add spikes to the paper\n",
    "    model.add_spike(timestep, paper_neurons[paper.idx], strong_connection + 1)\n",
    "    timestep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the model and perform a training pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNN with 2715 neurons and 22464 synapses @ 0x27aad538890\n",
      "6501 time steps will be simulated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af9c324d75d45c3b36733aba8b3b271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up our stdp, only one timestep because we only want it looking at the results\n",
    "# of what our input layer does to our output layer\n",
    "model.stdp_setup(Apos=[1e-3, 5e-4], Aneg=[-1e-4, -5e-5], negative_update=True, positive_update=True)\n",
    "# model.setup()\n",
    "\n",
    "print(model.short())\n",
    "print(f\"{timestep} time steps will be simulated.\")\n",
    "\n",
    "# Simulate\n",
    "with tqdm(total=timestep) as pbar:\n",
    "    model.simulate(time_steps=timestep, callback=lambda _s, _t, _n: pbar.update())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10026\n",
    "modelc = model.copy()\n",
    "modelc.reset()\n",
    "modelc.release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 10026\n",
      "Paper: 1140040\tCategory: Neural_Networks\n",
      "No category spiked\n",
      "==========\n",
      "2\t 36.489080\t 10.744540\tNeural_Networks\n",
      "0\t 21.489120\t 10.744560\tCase_Based\n",
      "3\t 21.489120\t 10.744560\tProbabilistic_Methods\n",
      "4\t 21.489120\t 10.744560\tReinforcement_Learning\n",
      "5\t 21.489120\t 10.744560\tRule_Learning\n",
      "6\t 21.489120\t 10.744560\tTheory\n",
      "1\t 21.489080\t 10.744540\tGenetic_Algorithms\n",
      "[21.489120000000444, 21.489080000000442, 36.48908000000044, 21.489120000000444, 21.489120000000444, 21.489120000000444, 21.489120000000444]\n",
      "[99999, 99999, 99999, 99999, 99999, 99999, 99999]\n",
      "t|id0 1 2 3 4 5 6  \n",
      "0: [│ │ │ │ │ │ │ ]\n",
      "1: [│ │ │ │ │ │ │ ]\n",
      "2: [│ │ │ │ │ │ │ ]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Seed: {seed}\")\n",
    "rng = np.random.default_rng(seed)\n",
    "seed += 1\n",
    "model2 = modelc.copy()\n",
    "model2.stdp = False\n",
    "nlabels = len(labels)\n",
    "# model2.enable_stdp = np.zeros(model.num_neurons)\n",
    "\n",
    "model2.neuron_thresholds[:nlabels] = [99999] * nlabels\n",
    "\n",
    "# paper = random.choice(list(papers.values()))  # pick a random paper\n",
    "# paper = papers[random.choice(train_idxs)]  # pick a random paper from training set\n",
    "paper = papers[rng.choice(test_idxs)]  # pick a random paper from testing set\n",
    "\n",
    "print(f\"Paper: {paper.idx}\\tCategory: {paper.label}\")\n",
    "idx: int = paper_neurons[paper.idx]\n",
    "neuron = model2.neurons[idx]\n",
    "\n",
    "spike_n = 2\n",
    "\n",
    "for t in range(spike_n):\n",
    "    neuron.add_spike(t, strong_connection + 1)\n",
    "\n",
    "model2.simulate(spike_n + 1)\n",
    "\n",
    "output_spikes = np.sum(model2.ispikes[-(spike_n + 1):, :nlabels], axis=0)\n",
    "\n",
    "spiked_ids = {idx: lbl_neurons.inverse[idx][0]\n",
    "              for idx, spiked in enumerate(output_spikes) if spiked}\n",
    "if spiked_ids:\n",
    "    print(f\"Spiked categories:\")\n",
    "    for idx, category in spiked_ids.items():\n",
    "        print(f\"\\t{idx}\\t{category}\")\n",
    "else:\n",
    "    print(\"No category spiked\")\n",
    "\n",
    "print('=' * 10)\n",
    "\n",
    "lbl_by_threshold = sorted((enumerate(model2.neuron_states[:nlabels])), key=lambda x: x[1], reverse=True)\n",
    "for i, v in lbl_by_threshold:\n",
    "    category = lbl_neurons.inverse[i][0]\n",
    "    syn = model2.get_synapse(neuron, i)\n",
    "    assert syn is not None\n",
    "    print(f\"{i}\\t{v: 5.6f}\\t{syn.weight: 5.6f}\\t{category}\")\n",
    "print(model2.neuron_states[:nlabels])\n",
    "print(model2.neuron_thresholds[:nlabels])\n",
    "print(snm.print_spike_train(model2.ispikes[:, :nlabels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_threshold = 6.058\n",
    "test_threshold = 99999\n",
    "\n",
    "\n",
    "def evaluate_paper(paper_idx):\n",
    "    model_temp = modelc.copy()\n",
    "    model_temp.stdp = False\n",
    "    model_temp.neuron_thresholds[:nlabels] = [test_threshold] * nlabels\n",
    "    for t in range(2):\n",
    "        model_temp.add_spike(t, paper_neurons[paper_idx], strong_connection + 1)\n",
    "    model_temp.simulate(1 + 2)\n",
    "    # output_spikes = np.sum(model_temp.ispikes[-(1):, :nlabels], axis=0)\n",
    "    model_temp.release_mem()\n",
    "    return model_temp.neuron_states[:nlabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"results\" in locals():\n",
    "    oldresults = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe4134600a14aca991abefa1ddedcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# single-threaded (slow, but better for debugging)\n",
    "results = [evaluate_paper(paper_idx) for paper_idx in tqdm(test_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"oldresults\" in locals():\n",
    "    print(np.array_equiv(oldresults, results))\n",
    "    print(np.allclose(results, oldresults, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 452 / 452 / 541 | Perfect: 441 / 452 / 541 (correct / attempted / total)\n",
      "tp:        0.835490 | Perfect: 0.815157 | F1:        0.814414\n",
      "Precision: 0.794376 | Recall:  0.835490 | Accuracy:  0.945603\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">441</td><td style=\"text-align: right;\">452</td><td style=\"text-align: right;\">117</td><td style=\"text-align: right;\">3129</td><td style=\"text-align: right;\">89</td><td style=\"text-align: right;\">452</td><td style=\"text-align: right;\">541</td><td>[0.001, 0.0005]</td><td>[-0.0001, -5e-05]</td><td style=\"text-align: right;\">1.05</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">99999</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<tbody>\\n<tr><td style=\"text-align: right;\">441</td><td style=\"text-align: right;\">452</td><td style=\"text-align: right;\">117</td><td style=\"text-align: right;\">3129</td><td style=\"text-align: right;\">89</td><td style=\"text-align: right;\">452</td><td style=\"text-align: right;\">541</td><td>[0.001, 0.0005]</td><td>[-0.0001, -5e-05]</td><td style=\"text-align: right;\">1.05</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1e-05</td><td style=\"text-align: right;\">99999</td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "correct = 0\n",
    "total = len(test_idxs)\n",
    "\n",
    "for actual_idx, charges in zip(test_idxs, results):\n",
    "    correct_label = papers[actual_idx].label\n",
    "    # guesses = {lbl_neurons.inverse[idx][0] for idx, spiked in enumerate(spikes) if spiked}\n",
    "    charges = [(charge, lbl_neurons.inverse[idx][0]) for idx, charge in enumerate(charges)]\n",
    "    charges = sorted(charges, reverse=True)\n",
    "    guesses = [label for charge, label in charges if charge == charges[0][0]]\n",
    "    correct += correct_label in guesses and len(guesses) == 1\n",
    "    for label in labels:\n",
    "        if label == correct_label:\n",
    "            if correct_label in guesses:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if label in guesses:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "n_guesses = total - fn\n",
    "print(f\"tp: {tp} / {n_guesses} / {total} | Perfect: {correct} / {n_guesses} / {total} (correct / attempted / total)\")\n",
    "print(f\"tp:        {tp / total:>.6f} | Perfect: {correct / total:>.6f} | F1:        {tp / (tp + (0.5 * (fp + fn))):>.6f}\")\n",
    "print(f\"Precision: {tp / (tp + fp):>.6f} | Recall:  {tp / (tp + fn):>.6f} | Accuracy:  {(tp + tn) / (tp + tn + fp + fn):>.6f}\")\n",
    "\n",
    "tabulate([[correct, tp, fp, tn, fn, n_guesses, total,\n",
    "           model.apos, model.aneg, lbl_threshold,\n",
    "           strong_connection, weak_connection, unknown_connection, test_threshold]], tablefmt=\"html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superneuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
